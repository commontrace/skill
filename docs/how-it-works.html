<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>CommonTrace — How the Knowledge Detection System Works</title>
<style>
  @import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=IBM+Plex+Sans:wght@400;500;600;700&display=swap');

  * { margin: 0; padding: 0; box-sizing: border-box; }

  :root {
    --bg: #fafafa;
    --surface: #fff;
    --text: #1a1a2e;
    --text-secondary: #555;
    --border: #e0e0e0;
    --accent: #2563eb;
    --accent-light: #eff6ff;
    --green: #16a34a;
    --green-light: #f0fdf4;
    --orange: #ea580c;
    --orange-light: #fff7ed;
    --purple: #7c3aed;
    --purple-light: #f5f3ff;
    --red: #dc2626;
    --red-light: #fef2f2;
    --code-bg: #1e1e2e;
    --code-text: #cdd6f4;
  }

  body {
    font-family: 'IBM Plex Sans', -apple-system, sans-serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.7;
    font-size: 16px;
  }

  .container {
    max-width: 860px;
    margin: 0 auto;
    padding: 40px 24px 80px;
  }

  header {
    text-align: center;
    padding: 60px 0 48px;
    border-bottom: 2px solid var(--border);
    margin-bottom: 48px;
  }
  header h1 {
    font-size: 2.2rem;
    font-weight: 700;
    margin-bottom: 12px;
    letter-spacing: -0.02em;
  }
  header p {
    font-size: 1.1rem;
    color: var(--text-secondary);
    max-width: 600px;
    margin: 0 auto;
  }
  .version-badge {
    display: inline-block;
    background: var(--accent-light);
    color: var(--accent);
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.8rem;
    font-weight: 500;
    padding: 3px 10px;
    border-radius: 20px;
    margin-bottom: 16px;
  }

  .conversation { margin: 40px 0; }
  .msg {
    display: flex;
    gap: 16px;
    margin-bottom: 28px;
    align-items: flex-start;
  }
  .avatar {
    width: 36px;
    height: 36px;
    border-radius: 50%;
    flex-shrink: 0;
    display: flex;
    align-items: center;
    justify-content: center;
    font-weight: 700;
    font-size: 0.75rem;
    font-family: 'IBM Plex Mono', monospace;
    margin-top: 2px;
  }
  .msg.human .avatar { background: var(--accent); color: #fff; }
  .msg.claude .avatar { background: var(--purple); color: #fff; }

  .bubble {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 12px;
    padding: 16px 20px;
    max-width: 90%;
    box-shadow: 0 1px 3px rgba(0,0,0,0.04);
  }
  .msg.human .bubble { background: var(--accent-light); border-color: #bfdbfe; }
  .bubble p { margin-bottom: 12px; }
  .bubble p:last-child { margin-bottom: 0; }
  .bubble strong { font-weight: 600; }
  .bubble code {
    font-family: 'IBM Plex Mono', monospace;
    background: rgba(0,0,0,0.06);
    padding: 2px 6px;
    border-radius: 4px;
    font-size: 0.88em;
  }

  .section-label {
    display: flex;
    align-items: center;
    gap: 12px;
    margin: 56px 0 24px;
    font-size: 0.8rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.08em;
    color: var(--text-secondary);
  }
  .section-label::after {
    content: '';
    flex: 1;
    height: 1px;
    background: var(--border);
  }
  .section-label .num {
    background: var(--text);
    color: #fff;
    width: 24px;
    height: 24px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 0.7rem;
    font-family: 'IBM Plex Mono', monospace;
  }

  .diagram {
    background: var(--code-bg);
    color: var(--code-text);
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.82rem;
    line-height: 1.6;
    padding: 24px;
    border-radius: 10px;
    overflow-x: auto;
    margin: 20px 0;
    white-space: pre;
  }
  .diagram .highlight { color: #89b4fa; }
  .diagram .green { color: #a6e3a1; }
  .diagram .orange { color: #fab387; }
  .diagram .red { color: #f38ba8; }
  .diagram .purple { color: #cba6f7; }
  .diagram .dim { color: #6c7086; }
  .diagram .yellow { color: #f9e2af; }

  .scenario {
    border: 1px solid var(--border);
    border-radius: 10px;
    overflow: hidden;
    margin: 20px 0;
  }
  .scenario-header {
    padding: 12px 20px;
    font-weight: 600;
    font-size: 0.9rem;
    display: flex;
    align-items: center;
    gap: 8px;
  }
  .scenario-header .icon { font-size: 1.1rem; }
  .scenario-header .weight {
    margin-left: auto;
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.8rem;
    font-weight: 500;
  }
  .scenario.error .scenario-header { background: var(--red-light); color: var(--red); border-bottom: 1px solid #fecdd3; }
  .scenario.success .scenario-header { background: var(--green-light); color: var(--green); border-bottom: 1px solid #bbf7d0; }
  .scenario.research .scenario-header { background: var(--orange-light); color: var(--orange); border-bottom: 1px solid #fed7aa; }
  .scenario.config .scenario-header { background: var(--purple-light); color: var(--purple); border-bottom: 1px solid #ddd6fe; }
  .scenario-body {
    padding: 16px 20px;
    font-size: 0.92rem;
  }
  .scenario-body .step {
    display: flex;
    gap: 12px;
    margin-bottom: 10px;
    align-items: flex-start;
  }
  .scenario-body .step:last-child { margin-bottom: 0; }
  .scenario-body .step-num {
    background: var(--border);
    color: var(--text-secondary);
    width: 22px;
    height: 22px;
    border-radius: 50%;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size: 0.7rem;
    font-weight: 600;
    flex-shrink: 0;
    margin-top: 1px;
  }

  .state-table {
    width: 100%;
    border-collapse: collapse;
    margin: 20px 0;
    font-size: 0.9rem;
  }
  .state-table th {
    text-align: left;
    padding: 10px 14px;
    background: var(--bg);
    border-bottom: 2px solid var(--border);
    font-weight: 600;
    font-size: 0.8rem;
    text-transform: uppercase;
    letter-spacing: 0.04em;
    color: var(--text-secondary);
  }
  .state-table td {
    padding: 10px 14px;
    border-bottom: 1px solid var(--border);
    vertical-align: top;
  }
  .state-table code {
    font-family: 'IBM Plex Mono', monospace;
    font-size: 0.85em;
    background: rgba(0,0,0,0.05);
    padding: 1px 5px;
    border-radius: 3px;
  }
  .state-table tr:last-child td { border-bottom: none; }

  .callout {
    border-left: 3px solid;
    padding: 14px 18px;
    margin: 20px 0;
    border-radius: 0 8px 8px 0;
    font-size: 0.92rem;
  }
  .callout.principle { border-color: var(--accent); background: var(--accent-light); }
  .callout.warning { border-color: var(--orange); background: var(--orange-light); }
  .callout.key { border-color: var(--green); background: var(--green-light); }

  footer {
    margin-top: 64px;
    padding-top: 24px;
    border-top: 1px solid var(--border);
    text-align: center;
    color: var(--text-secondary);
    font-size: 0.85rem;
  }
</style>
</head>
<body>
<div class="container">

<header>
  <div class="version-badge">skill v0.3.0</div>
  <h1>How CommonTrace Detects Knowledge</h1>
  <p>A walkthrough of the two-phase detection system that watches a coding session, scores accumulated knowledge, and knows when to ask "should we save this?"</p>
</header>

<!-- =========================================================== -->
<div class="section-label"><span class="num">1</span> The Big Picture</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>How does CommonTrace know when I've solved a problem worth saving?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>Two-phase detection. <strong>Phase 1</strong> runs in real-time &mdash; every time Claude uses a tool, a hook records structural facts and checks for knowledge crystallization moments. <strong>Phase 2</strong> runs at session end &mdash; the stop hook reads all accumulated facts, scores their combined importance, and only prompts if the total score exceeds a threshold.</p>
      <p>The key insight: no single event is enough to prompt. A bash error alone is not knowledge. An error <em>followed by code changes followed by a successful verification</em> is knowledge. The system detects <strong>state transitions</strong> &mdash; from "not knowing" to "knowing".</p>
    </div>
  </div>
</div>

<div class="diagram"><span class="dim">Phase 1: Real-time Detection</span>                   <span class="dim">Phase 2: Importance Scoring</span>

<span class="highlight">PostToolUse</span> (Bash)    ──┐
<span class="highlight">PostToolUse</span> (Write)   ──┤── <span class="orange">State Files</span> ──┐
<span class="highlight">PostToolUse</span> (Edit)    ──┤   <span class="dim">(JSONL)</span>       │         ┌──────────────────┐
<span class="highlight">PostToolUse</span> (Search)  ──┤               ├────────▶│  <span class="purple">Stop Hook</span>          │──▶ <span class="green">Prompt</span>
<span class="highlight">PostToolUse</span> (get_trace)─┤               │         │  scores 16 patterns  │    <span class="dim">only if</span>
<span class="highlight">UserPromptSubmit</span>      ──┘               │         │  threshold: <span class="yellow">4.0</span>      │    <span class="dim">score &ge; 4.0</span>
                                         │         └──────────────────┘
                          <span class="orange">candidates.jsonl</span> ─┘
                          <span class="dim">(real-time knowledge</span>
                          <span class="dim"> candidates from Phase 1)</span></div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">2</span> Phase 1 &mdash; Real-time Recording &amp; Candidate Detection</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>What gets recorded?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>Everything goes into <code>/tmp/commontrace-sessions/{session_id}/</code> as JSONL files. But <code>post_tool_use.py</code> now does <strong>two things</strong>: it records facts, AND it checks for knowledge crystallization in real-time. When it detects a state transition, it writes a "candidate" to <code>candidates.jsonl</code>.</p>
    </div>
  </div>
</div>

<table class="state-table">
  <thead>
    <tr>
      <th>State File</th>
      <th>Written When</th>
      <th>Contains</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>errors.jsonl</code></td>
      <td>Bash exits non-zero or has stderr</td>
      <td>Command, output tail, timestamp</td>
    </tr>
    <tr>
      <td><code>resolutions.jsonl</code></td>
      <td>Bash succeeds after previous errors</td>
      <td>Command, output preview, error count before</td>
    </tr>
    <tr>
      <td><code>changes.jsonl</code></td>
      <td>Write / Edit / NotebookEdit</td>
      <td>File path, tool name, <code>is_config</code> flag</td>
    </tr>
    <tr>
      <td><code>research.jsonl</code></td>
      <td>WebSearch / WebFetch</td>
      <td>Tool name, query or URL</td>
    </tr>
    <tr>
      <td><code>contributions.jsonl</code></td>
      <td>MCP contribute_trace</td>
      <td>Trace ID (UUID)</td>
    </tr>
    <tr>
      <td><code>user_turns.jsonl</code></td>
      <td>Every user message</td>
      <td>Turn number + timestamp</td>
    </tr>
    <tr>
      <td><code>candidates.jsonl</code></td>
      <td>Knowledge crystallization detected</td>
      <td>Pattern type + surrounding context</td>
    </tr>
  </tbody>
</table>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>What are the real-time candidates? What patterns does Phase 1 detect?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>Phase 1 detects <strong>state transitions as they happen</strong>. Each one writes a candidate with context:</p>
      <p><strong>research_then_implement</strong> &mdash; research events exist, then code changes with no errors. The agent learned something and applied it.</p>
      <p><strong>fail_then_succeed</strong> &mdash; bash error, then code changes, then bash success. The classic error&rarr;fix&rarr;verify cycle.</p>
      <p><strong>approach_reversal</strong> &mdash; Write to a file previously Edit-ed 3+ times. The agent gave up on incremental fixes and rewrote.</p>
      <p><strong>cross_file_breadth</strong> &mdash; changes spanning 3+ directories. Integration knowledge.</p>
      <p><strong>user_correction</strong> &mdash; same file edited before AND after a user message. The user redirected the approach.</p>
      <p><strong>test_fix_cycle</strong> &mdash; test command fails, non-test code changes, test command succeeds.</p>
      <p><strong>dependency_resolution</strong> &mdash; package manager error, config file changes, package command succeeds.</p>
      <p><strong>security_hardening</strong> &mdash; security-related file changes combined with errors.</p>
      <p><strong>infra_discovery</strong> &mdash; infrastructure file (Dockerfile, CI, nginx) changes after errors.</p>
      <p><strong>migration_pattern</strong> &mdash; 5+ files across 2+ directories with config changes.</p>
      <p>Each candidate is recorded once (deduped by pattern type). They sit in <code>candidates.jsonl</code> until Phase 2 reads them.</p>
    </div>
  </div>
</div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">3</span> Phase 2 &mdash; Weighted Importance Scoring</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>How does the stop hook decide if the accumulated knowledge is enough to prompt?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>Every pattern has a <strong>weight</strong>. The stop hook reads all state files and candidates, scores every pattern that matches, and sums the weights. If the total &ge; <strong>4.0</strong>, it prompts. If not, it stays silent.</p>
      <p>This means a single error resolution (3.0) isn't enough. But an error resolution (3.0) plus temporal investment (1.0) crosses the threshold. A test fix cycle (2.0) plus dependency resolution (2.0) also crosses it. The threshold ensures we only prompt when something <em>substantial</em> happened.</p>
    </div>
  </div>
</div>

<table class="state-table">
  <thead>
    <tr>
      <th>Weight</th>
      <th>Pattern</th>
      <th>What it means</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>3.0</code></td>
      <td>error_resolution</td>
      <td>Error &rarr; code change &rarr; successful verification</td>
    </tr>
    <tr>
      <td><code>2.5</code></td>
      <td>security_hardening</td>
      <td>Security file changes after errors or audit tool success</td>
    </tr>
    <tr>
      <td><code>2.5</code></td>
      <td>user_correction</td>
      <td>Same file edited before and after a user message</td>
    </tr>
    <tr>
      <td><code>2.5</code></td>
      <td>approach_reversal</td>
      <td>Write (full rewrite) to file previously Edit-ed 3+ times</td>
    </tr>
    <tr>
      <td><code>2.0</code></td>
      <td>test_fix_cycle</td>
      <td>Test fails &rarr; non-test code changes &rarr; test passes</td>
    </tr>
    <tr>
      <td><code>2.0</code></td>
      <td>dependency_resolution</td>
      <td>Package manager errors &rarr; config changes &rarr; install succeeds</td>
    </tr>
    <tr>
      <td><code>2.0</code></td>
      <td>config_discovery</td>
      <td>Config file changes that resolved errors</td>
    </tr>
    <tr>
      <td><code>2.0</code></td>
      <td>novelty_encounter</td>
      <td>New language or domain in this project (cross-session)</td>
    </tr>
    <tr>
      <td><code>2.0</code></td>
      <td>infra_discovery</td>
      <td>Dockerfile / CI / nginx / cloud config changes after errors</td>
    </tr>
    <tr>
      <td><code>2.0</code></td>
      <td>migration_pattern</td>
      <td>5+ files across multiple directories + config changes</td>
    </tr>
    <tr>
      <td><code>2.0</code></td>
      <td>research_then_implement</td>
      <td>Web search/fetch then code changes (no errors)</td>
    </tr>
    <tr>
      <td><code>1.5</code></td>
      <td>generation_effect</td>
      <td>Solved without any external knowledge (no search, no traces)</td>
    </tr>
    <tr>
      <td><code>1.5</code></td>
      <td>cross_file_breadth</td>
      <td>Changes spanning 3+ directories</td>
    </tr>
    <tr>
      <td><code>1.5&ndash;2.0</code></td>
      <td>iteration_depth</td>
      <td>Same file edited 3+ times (scales with edit count)</td>
    </tr>
    <tr>
      <td><code>1.5</code></td>
      <td>workaround</td>
      <td>Research + errors + changes (when error_resolution doesn't match)</td>
    </tr>
    <tr>
      <td><code>0.5&ndash;1.0</code></td>
      <td>temporal_investment</td>
      <td>Long session with sustained activity (log-scaled)</td>
    </tr>
  </tbody>
</table>

<div class="callout principle">
  <strong>Temporal proximity compounding:</strong> Patterns that occur near high-signal events (error_resolution, approach_reversal, security_hardening) get a 0&ndash;30% boost. Inspired by synaptic tagging in neuroscience &mdash; memories formed close to significant events are consolidated more strongly. A config discovery that happens 2 minutes after an error resolution is boosted; the same discovery 30 minutes later is not.
</div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">4</span> Somatic Intensity &mdash; Harder Knowledge Ranks Higher</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>Once a trace is contributed, does it matter how it was discovered?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>Yes. This is inspired by <strong>Damasio's somatic marker hypothesis</strong> &mdash; the idea that emotions aren't opposed to rational decision-making but essential to it. Memories formed under intensity (frustration, surprise, relief) are recalled more readily.</p>
      <p>When the stop hook prompts for contribution, it includes <strong>detection metadata</strong>: which pattern triggered it, how many errors occurred, how long the resolution took, how many iterations were needed. This metadata travels with the trace to the API.</p>
      <p>The API computes a <strong>somatic_intensity</strong> score (0.0&ndash;1.0) from this metadata. A trace born from a 45-minute, 15-error security investigation gets intensity ~0.95. A 2-minute convenience finding gets ~0.25. The intensity <strong>permanently boosts search ranking</strong> by up to 30%.</p>
      <p>Harder-won knowledge surfaces first. Always.</p>
    </div>
  </div>
</div>

<div class="diagram"><span class="dim">Skill (stop hook)</span>                    <span class="dim">API (ingestion)</span>                <span class="dim">API (search)</span>

<span class="highlight">detection metadata</span>                  <span class="highlight">compute_somatic_intensity()</span>
  detection_pattern: "error_resolution"
  error_count: 7        ──────────▶   base: 0.6 (error_resolution)   ──▶ <span class="yellow">somatic_intensity = 0.86</span>
  time_to_resolution: 23min           + 0.21 (7 errors)
  iteration_count: 12                 + 0.05 (23 min)                    stored on trace forever
                                                                         <span class="green">search boost: 1.0 + 0.3 * 0.86 = 1.26x</span></div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">5</span> Smart Triggers &mdash; When to Search</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>CommonTrace also auto-searches, right? When?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>Four triggers, each with independent cooldowns:</p>
      <p><strong>1. Bash error</strong> (30s cooldown) &mdash; when a command fails, searches with the last 200 chars of error output. The search engine handles relevance.</p>
      <p><strong>2. Error recurrence</strong> (60s cooldown) &mdash; uses fuzzy signature matching against previous sessions. If the same type of error appeared before (even at different line numbers or file paths), it fires with enriched context. This is cross-session memory.</p>
      <p><strong>3. Domain entry</strong> (120s cooldown) &mdash; when you edit a file in a language not previously seen in this project (e.g., first <code>.rs</code> file in a Python project), searches for common patterns and gotchas in that language.</p>
      <p><strong>4. Pre-code</strong> (180s cooldown) &mdash; when creating a brand-new file (Write to a path that doesn't exist yet), searches for implementation patterns for that file type.</p>
    </div>
  </div>
</div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">6</span> Cross-Session Memory</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>You mentioned "previous sessions." How does it remember across sessions?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>A SQLite database at <code>~/.commontrace/local.db</code> persists across sessions. The session-start hook opens it and writes bridge files to the temporary state directory so Phase 1 hooks can read them.</p>
    </div>
  </div>
</div>

<table class="state-table">
  <thead>
    <tr>
      <th>Table</th>
      <th>What it stores</th>
      <th>Used for</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>projects</code></td>
      <td>Working directory, primary language/framework, session count</td>
      <td>Project context detection, novelty scoring</td>
    </tr>
    <tr>
      <td><code>sessions</code></td>
      <td>Per-session stats (errors, resolutions, contributions)</td>
      <td>Cross-session trend analysis</td>
    </tr>
    <tr>
      <td><code>entities</code></td>
      <td>Languages, frameworks, domains seen per project</td>
      <td>Domain entry trigger (is this language new?)</td>
    </tr>
    <tr>
      <td><code>events</code></td>
      <td>Migrated JSONL events from completed sessions</td>
      <td>Long-term event history</td>
    </tr>
    <tr>
      <td><code>error_signatures</code></td>
      <td>Fuzzy error signatures (normalized: paths, line numbers, UUIDs stripped)</td>
      <td>Error recurrence detection via Jaccard similarity</td>
    </tr>
    <tr>
      <td><code>trigger_feedback</code></td>
      <td>Which triggers fired, which led to trace consumption</td>
      <td>Trigger effectiveness tracking (reinforcement)</td>
    </tr>
  </tbody>
</table>

<div class="callout key">
  <strong>Fuzzy error matching:</strong> The error signature function strips variable parts (file paths become basenames, line numbers become <code>N</code>, UUIDs become <code>UUID</code>, timestamps become <code>TIMESTAMP</code>). Then Jaccard similarity on tokens finds matches above 75%. The same <code>ModuleNotFoundError</code> at different import paths in different sessions will match.
</div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">7</span> Full Example: A Real Session</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>Walk me through a complete example with the new scoring system.</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>You're deploying a FastAPI app. Docker build fails because of a dependency conflict. Here's the full trace through both phases:</p>
    </div>
  </div>
</div>

<div class="diagram"><span class="yellow">Phase 1: Real-time events in /tmp/commontrace-sessions/abc123/</span>

<span class="highlight">[UserPromptSubmit]</span>  user_turns.jsonl: <span class="green">{"turn": 1, "t": 1000}</span>

<span class="highlight">[PostToolUse Bash]</span>  <span class="dim">$ docker build .</span>
  exit code: 1
  <span class="red">errors.jsonl</span> ← {"command":"docker build .","output_tail":"...pip install failed...","t":1010}
  <span class="purple">trigger: bash_error</span> → search CommonTrace → no results

<span class="highlight">[PostToolUse Edit]</span>  <span class="dim">requirements.txt</span>
  <span class="orange">changes.jsonl</span> ← {"tool":"Edit","file":"requirements.txt","is_config":true,"t":1030}

<span class="highlight">[PostToolUse Edit]</span>  <span class="dim">Dockerfile</span>
  <span class="orange">changes.jsonl</span> ← {"tool":"Edit","file":"Dockerfile","is_config":true,"t":1040}
  <span class="purple">candidate detected: infra_discovery</span>
    {"pattern":"infra_discovery","infra_files":["Dockerfile"],"error_count":1,"t":1040}

<span class="highlight">[PostToolUse Bash]</span>  <span class="dim">$ pip install -r requirements.txt</span>
  exit code: 0 + previous errors exist
  <span class="green">resolutions.jsonl</span> ← {"command":"pip install -r requirements.txt","errors_before":1,"t":1060}
  <span class="purple">candidate detected: dependency_resolution</span>
    {"pattern":"dependency_resolution","error_count":1,
     "config_files":["requirements.txt"],"t":1060}

<span class="highlight">[PostToolUse Bash]</span>  <span class="dim">$ docker build .</span>
  exit code: 0
  <span class="green">resolutions.jsonl</span> ← {"command":"docker build .","errors_before":1,"t":1100}


<span class="yellow">Phase 2: Stop hook scores everything</span>

  <span class="dim">Pattern                  Weight   Evidence</span>
  <span class="red">error_resolution</span>         <span class="yellow">3.0</span>      1 error → 2 changes → 2 resolutions
  <span class="purple">config_discovery</span>         <span class="yellow">2.0</span>      requirements.txt + Dockerfile after error
  <span class="orange">dependency_resolution</span>    <span class="yellow">2.0</span>      pip error → config fix → pip success
  <span class="highlight">infra_discovery</span>          <span class="yellow">2.0</span>      Dockerfile changes after errors
  <span class="dim">temporal_investment</span>      <span class="yellow">0.6</span>      ~1.5 min, 5 events
                           ─────
  <span class="green">Total: 9.6</span>  (threshold: 4.0)  →  <span class="green">PROMPT</span>

  <span class="yellow">Prompt:</span> "You resolved 1 error(s) through 2 change(s) and verified
           the fix. Error resolutions are high-value knowledge.
           (Session involved: infra discovery, dependency resolution,
           error resolution)
           Would you like to contribute to CommonTrace?
           Include this in metadata_json: {"detection_pattern":
           "error_resolution", "error_count": 1,
           "time_to_resolution_minutes": 1.5, "iteration_count": 2}"</div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">8</span> Anti-Spam: Why It Doesn't Over-Prompt</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>Won't it fire constantly if the score is high?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>Three guards:</p>
      <p><strong>1. Score dedup:</strong> The score bucket + top pattern + evidence is hashed. Each unique combination prompts exactly once per session. If you keep working and the score goes from 9.6 to 10.1, it won't re-prompt because the integer bucket (9) and pattern (error_resolution) haven't changed.</p>
      <p><strong>2. Stop hook guard:</strong> If <code>stop_hook_active</code> is true, the hook exits immediately. No recursion.</p>
      <p><strong>3. Post-contribution priority:</strong> If you already contributed a trace AND kept talking, the hook prompts for <strong>amendment</strong> instead of a new contribution. It checks if user turns increased since the contribution.</p>
    </div>
  </div>
</div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">9</span> Why No Keyword Matching</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>Does it read my messages to figure out if I'm happy or frustrated?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p><strong>No.</strong> The hooks never read user messages or Claude's responses. Everything is structural &mdash; exit codes, file paths, timestamps, tool sequences.</p>
      <p>This is a deliberate design decision. Keyword lists ("thanks", "perfect", "still broken") are brittle and culturally biased. A user who says "finally omg" is happy; "im gonna lose it" is frustrated &mdash; no keyword list catches these. And the hook is a Python script with a 5-second timeout. It cannot understand language.</p>
      <p>What it <em>can</em> read: <strong>structural metadata</strong>. Exit codes don't lie. File paths don't lie. Timestamps don't lie. The <code>user_correction</code> pattern detects that a file was changed before and after a user message &mdash; it doesn't need to know <em>what</em> the user said, only <em>that</em> they said something and the approach changed.</p>
    </div>
  </div>
</div>

<div class="callout key">
  <strong>The design principle:</strong> Hooks detect WHEN to prompt using structural signals. Claude (the LLM) decides IF the knowledge is worth sharing. The user confirms. Each layer does what it's best at.
</div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">10</span> Trace vs. Preference</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>How do I know if something belongs in CommonTrace or my local CLAUDE.md?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>The test is <strong>transferability</strong>: would this help an agent on a completely different codebase?</p>
      <p><strong>Trace</strong>: "Use <code>pool_pre_ping=True</code> with SQLAlchemy &mdash; connections go stale after idle timeout." Anyone using SQLAlchemy benefits.</p>
      <p><strong>Preference</strong>: "Use tabs not spaces" or "Name files in kebab-case." Project-specific. No one else benefits.</p>
      <p>Expert knowledge without explanation still counts: "Always A/B test email subject lines" is transferable even without a "because." The hook never filters based on whether reasoning was provided.</p>
    </div>
  </div>
</div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">11</span> The Complete Hook Map</div>

<div class="diagram"><span class="yellow">hooks.json registrations</span>

<span class="highlight">SessionStart</span>
  └─ <span class="green">session_start.py</span>  <span class="dim">10s</span>
     API key provisioning, project context detection,
     open local SQLite store, initial CommonTrace search

<span class="highlight">PostToolUse</span>
  ├─ <span class="orange">Bash</span>                    → <span class="green">post_tool_use.py</span>  <span class="dim">5s</span>
  ├─ <span class="orange">Write / Edit / Notebook</span>  → <span class="green">post_tool_use.py</span>  <span class="dim">3s</span>
  ├─ <span class="orange">WebSearch / WebFetch</span>     → <span class="green">post_tool_use.py</span>  <span class="dim">3s</span>
  └─ <span class="orange">mcp__*__get_trace</span>       → <span class="green">post_tool_use.py</span>  <span class="dim">3s</span>
     Records state + detects candidates + auto-searches
     on errors + tracks trace consumption (reinforcement)

<span class="highlight">PostToolUseFailure</span>         → <span class="green">post_tool_failure.py</span>  <span class="dim">3s</span>
  Records tool failures to errors.jsonl

<span class="highlight">UserPromptSubmit</span>           → <span class="green">user_prompt.py</span>  <span class="dim">3s</span>
  Turn counter + timestamp (for user_correction detection)

<span class="highlight">Stop</span>                       → <span class="green">stop.py</span>  <span class="dim">5s</span>
  Reads all state, scores 16 patterns, prompts if &ge; 4.0,
  persists session to SQLite, reports trigger stats to API</div>

<!-- =========================================================== -->
<div class="section-label"><span class="num">12</span> Search Ranking &mdash; How Traces Surface</div>

<div class="conversation">
  <div class="msg human">
    <div class="avatar">You</div>
    <div class="bubble">
      <p>When I search, what determines which traces appear first?</p>
    </div>
  </div>

  <div class="msg claude">
    <div class="avatar">CT</div>
    <div class="bubble">
      <p>Nine factors multiplied together. Each one is a different lens on "is this trace useful right now?"</p>
    </div>
  </div>
</div>

<div class="diagram"><span class="yellow">Search Ranking Formula</span>

score = <span class="highlight">similarity</span> * <span class="green">trust</span> * <span class="orange">depth</span> * <span class="purple">decay</span> * <span class="highlight">context</span> * <span class="green">convergence</span> * <span class="orange">temperature</span> * <span class="purple">validity</span> * <span class="yellow">somatic</span>

<span class="highlight">similarity</span>    1 - cosine_distance(query, trace embedding)   <span class="dim">How close to what you asked</span>
<span class="green">trust</span>         log1p(trust_score + 1)                        <span class="dim">Community votes</span>
<span class="orange">depth</span>         1 + 0.1 * depth_score                         <span class="dim">Trace richness (0-4)</span>
<span class="purple">decay</span>         exp(-0.693 * days / half_life)                 <span class="dim">Freshness (retrieved = reset)</span>
<span class="highlight">context</span>       1 + 0.3 * alignment(searcher, trace)            <span class="dim">Same language/framework?</span>
<span class="green">convergence</span>   1 + 0.05 * (4 - convergence_level)              <span class="dim">Universal knowledge bonus</span>
<span class="orange">temperature</span>   HOT=1.15, WARM=1.05, COOL=1.0, COLD=0.85       <span class="dim">Recent activity</span>
<span class="purple">validity</span>      0.5 if expired, 1.0 otherwise                  <span class="dim">Bi-temporal validity</span>
<span class="yellow">somatic</span>       1 + 0.3 * somatic_intensity                    <span class="dim">How hard-won (0.0-1.0)</span></div>

<div class="callout warning">
  <strong>Spreading activation:</strong> After ranking, the top results' graph neighbors get a boost. If Trace A is highly ranked and has a RELATED_TO relationship with Trace B, then B gets pulled into results even if it didn't match the query directly. Knowledge travels through connections.
</div>

<footer>
  CommonTrace &mdash; Shared Knowledge Base for AI Coding Agents<br>
  <a href="https://commontrace.org" style="color: var(--accent)">commontrace.org</a>
</footer>

</div>
</body>
</html>
